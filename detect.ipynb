{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "                                                    text  generated\n",
      "10607  If I were a scientist at NASA I will not belie...          0\n",
      "27769  \"America's love affair with it's vehicles seem...          0\n",
      "7663   There is a great challenge when it comes to ex...          0\n",
      "2916   Cell phones have become very popular over the ...          0\n",
      "8409   From the research and development of Dr. Huang...          0\n",
      "\n",
      "Test Data:\n",
      "                                                    text  generated\n",
      "17004  I would agree with Emerson's in this world be ...          0\n",
      "14459  Advice is wonderful and helpful to everyone. S...          0\n",
      "28492  I think that limiting car usage is great for t...          0\n",
      "10134  Nobody know how the face got on mars because w...          0\n",
      "23657  The student has studied lot of subjects in the...          1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Read\n",
    "data = pd.read_csv('Training_Essay_data.csv')\n",
    "\n",
    "#Split to train and test\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training Data:\")\n",
    "print(train_data.head())\n",
    "\n",
    "print(\"\\nTest Data:\")\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 5.447241673389782e-07\n",
      "Testing MSE: 0.10461939701778478\n",
      "Training Accuracy: 0.999997731978449\n",
      "Testing Accuracy: 0.5613841936268535\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Prepare the data\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_data['text'])\n",
    "y_train = train_data['generated']\n",
    "X_test = vectorizer.transform(test_data['text'])\n",
    "y_test = test_data['generated']\n",
    "\n",
    "# Initialize the model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on training and testing data\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate training and testing accuracy\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Training MSE: {train_mse}\")\n",
    "print(f\"Testing MSE: {test_mse}\")\n",
    "\n",
    "# Calculate accuracy\n",
    "train_accuracy = model.score(X_train, y_train)\n",
    "test_accuracy = model.score(X_test, y_test)\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.003844954539090395\n",
      "Epoch 2/10, Loss: 0.004386894404888153\n",
      "Epoch 3/10, Loss: 3.540900070220232e-05\n",
      "Epoch 4/10, Loss: 8.149359200615436e-05\n",
      "Epoch 5/10, Loss: 3.334660505061038e-05\n",
      "Epoch 6/10, Loss: 3.442191518843174e-05\n",
      "Epoch 7/10, Loss: 2.8987047699047253e-05\n",
      "Epoch 8/10, Loss: 2.604674045869615e-05\n",
      "Epoch 9/10, Loss: 3.419326458242722e-05\n",
      "Epoch 10/10, Loss: 1.0277997716912068e-05\n",
      "Training MSE: 3.342935441346634e-05\n",
      "Testing MSE: 0.0023896746468276887\n",
      "Training Accuracy: 0.9999665706455866\n",
      "Testing Accuracy: 0.9976103253531723\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train.toarray(), dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "X_test_tensor = torch.tensor(X_test.toarray(), dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Create DataLoader for training and testing data\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define a simple neural network model\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "input_dim = X_train.shape[1]\n",
    "model = SimpleNN(input_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_train_pred = model(X_train_tensor).numpy()\n",
    "    y_test_pred = model(X_test_tensor).numpy()\n",
    "\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Training MSE: {train_mse}\")\n",
    "print(f\"Testing MSE: {test_mse}\")\n",
    "\n",
    "# Calculate accuracy\n",
    "train_accuracy = 1 - train_mse\n",
    "test_accuracy = 1 - test_mse\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Check if GPU is available\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "# Initialize the text generation pipeline\n",
    "generator = pipeline('text-generation', model='gpt2', device=device)\n",
    "\n",
    "# # Generate sentences\n",
    "generated_texts = []\n",
    "# for _ in range(20):  # Generate 50 longer texts\n",
    "#     generated = generator(\"Once upon a time\", max_length=300, num_return_sequences=1)\n",
    "#     generated_texts.append(generated[0]['generated_text'])\n",
    "\n",
    "# Generate additional sentences with different topics\n",
    "topics = [\n",
    "    \"The future of renewable energy\",\n",
    "    \"Advancements in artificial intelligence\",\n",
    "    \"The impact of climate change on agriculture\",\n",
    "    \"The role of technology in education\",\n",
    "    \"The benefits of remote work\",\n",
    "    \"The importance of mental health awareness\",\n",
    "    \"The evolution of electric vehicles\",\n",
    "    \"The significance of biodiversity conservation\",\n",
    "    \"The challenges of space exploration\",\n",
    "    \"The influence of social media on society\"\n",
    "]\n",
    "\n",
    "for topic in topics:\n",
    "    for _ in range(2):  #2 long texts per topic for testing\n",
    "        generated = generator(topic, max_length=300, num_return_sequences=1)\n",
    "        generated_texts.append(generated[0]['generated_text'])\n",
    "\n",
    "# Create a DataFrame\n",
    "generated_df = pd.DataFrame({'text': generated_texts, 'generated': [1] * len(generated_texts)})\n",
    "\n",
    "# Save to CSV\n",
    "generated_df.to_csv('huggingFaceGenerated.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HuggingFace Generated Data MSE: 0.09664390292520611\n",
      "HuggingFace Generated Data Accuracy: 0.9033560970747939\n"
     ]
    }
   ],
   "source": [
    "# Load the huggingFaceGenerated.csv data\n",
    "huggingface_data = pd.read_csv('huggingFaceGenerated.csv')\n",
    "\n",
    "# Preprocess the text data using the same TfidfVectorizer\n",
    "X_huggingface = vectorizer.transform(huggingface_data['text'])\n",
    "y_huggingface = huggingface_data['generated']\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "X_huggingface_tensor = torch.tensor(X_huggingface.toarray(), dtype=torch.float32)\n",
    "y_huggingface_tensor = torch.tensor(y_huggingface.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Use the trained model to make predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_huggingface_pred = model(X_huggingface_tensor).numpy()\n",
    "\n",
    "# Calculate MSE and accuracy\n",
    "huggingface_mse = mean_squared_error(y_huggingface, y_huggingface_pred)\n",
    "huggingface_accuracy = 1 - huggingface_mse\n",
    "\n",
    "print(f\"HuggingFace Generated Data MSE: {huggingface_mse}\")\n",
    "print(f\"HuggingFace Generated Data Accuracy: {huggingface_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: The future of renewable energy...\n",
      "Confidence Score: 0.9725349545478821\n",
      "\n",
      "Text: The future of renewable energy...\n",
      "Confidence Score: 0.9209951758384705\n",
      "\n",
      "Text: Advancements in artificial intelligence is...\n",
      "Confidence Score: 0.8973162770271301\n",
      "\n",
      "Text: Advancements in artificial intelligence offer...\n",
      "Confidence Score: 0.9362174868583679\n",
      "\n",
      "Text: The impact of climate change...\n",
      "Confidence Score: 0.9565624594688416\n",
      "\n",
      "Text: The impact of climate change...\n",
      "Confidence Score: 0.9926563501358032\n",
      "\n",
      "Text: The role of technology in...\n",
      "Confidence Score: 0.9898195266723633\n",
      "\n",
      "Text: The role of technology in...\n",
      "Confidence Score: 0.9526947140693665\n",
      "\n",
      "Text: The benefits of remote work...\n",
      "Confidence Score: 0.19596216082572937\n",
      "\n",
      "Text: The benefits of remote work...\n",
      "Confidence Score: 0.982722282409668\n",
      "\n",
      "Text: The importance of mental health...\n",
      "Confidence Score: 0.8429860472679138\n",
      "\n",
      "Text: The importance of mental health...\n",
      "Confidence Score: 0.9514722228050232\n",
      "\n",
      "Text: The evolution of electric vehicles,...\n",
      "Confidence Score: 0.8754194378852844\n",
      "\n",
      "Text: The evolution of electric vehicles...\n",
      "Confidence Score: 0.9424371123313904\n",
      "\n",
      "Text: The significance of biodiversity conservation...\n",
      "Confidence Score: 0.9589130878448486\n",
      "\n",
      "Text: The significance of biodiversity conservation...\n",
      "Confidence Score: 0.94134122133255\n",
      "\n",
      "Text: The challenges of space exploration...\n",
      "Confidence Score: 0.4690365791320801\n",
      "\n",
      "Text: The challenges of space exploration...\n",
      "Confidence Score: 0.7638193964958191\n",
      "\n",
      "Text: The influence of social media...\n",
      "Confidence Score: 0.0669122189283371\n",
      "\n",
      "Text: The influence of social media...\n",
      "Confidence Score: 0.9702525734901428\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate confidence scores for each text in huggingFaceGenerated\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_huggingface_pred = model(X_huggingface_tensor).numpy()\n",
    "\n",
    "# Print out the confidence score for each text\n",
    "for text, score in zip(huggingface_data['text'], y_huggingface_pred):\n",
    "    print(f\"Text: {' '.join(text.split()[:5])}...\\nConfidence Score: {score[0]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusing Generated Data MSE: 0.07018654076773231\n",
      "Confusing Generated Data Accuracy: 0.9298134592322677\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "def introduce_noise(text):\n",
    "    words = text.split()\n",
    "    noisy_text = []\n",
    "    for word in words:\n",
    "        if random.random() < 0.1:  # 10% chance to introduce noise\n",
    "            noisy_text.append(''.join(random.choices(string.ascii_letters, k=len(word))))\n",
    "        else:\n",
    "            noisy_text.append(word)\n",
    "    return ' '.join(noisy_text)\n",
    "\n",
    "def replace_with_synonyms(text):\n",
    "    synonyms = {\n",
    "        \"future\": [\"prospect\", \"outlook\", \"horizon\"],\n",
    "        \"energy\": [\"power\", \"vigor\", \"force\"],\n",
    "        \"advancements\": [\"progress\", \"improvements\", \"developments\"],\n",
    "        \"impact\": [\"effect\", \"influence\", \"consequence\"],\n",
    "        \"technology\": [\"tech\", \"innovation\", \"machinery\"],\n",
    "        \"education\": [\"schooling\", \"instruction\", \"teaching\"],\n",
    "        \"benefits\": [\"advantages\", \"gains\", \"profits\"],\n",
    "        \"mental\": [\"psychological\", \"emotional\", \"cognitive\"],\n",
    "        \"health\": [\"well-being\", \"fitness\", \"condition\"],\n",
    "        \"evolution\": [\"development\", \"progression\", \"advancement\"],\n",
    "        \"vehicles\": [\"cars\", \"automobiles\", \"transport\"],\n",
    "        \"significance\": [\"importance\", \"meaning\", \"value\"],\n",
    "        \"biodiversity\": [\"variety of life\", \"ecosystem diversity\", \"species richness\"],\n",
    "        \"challenges\": [\"difficulties\", \"obstacles\", \"hurdles\"],\n",
    "        \"exploration\": [\"investigation\", \"examination\", \"discovery\"],\n",
    "        \"influence\": [\"effect\", \"impact\", \"sway\"],\n",
    "        \"social\": [\"societal\", \"community\", \"public\"],\n",
    "        \"media\": [\"press\", \"news\", \"communication\"]\n",
    "    }\n",
    "    words = text.split()\n",
    "    synonym_text = []\n",
    "    for word in words:\n",
    "        if word in synonyms:\n",
    "            synonym_text.append(random.choice(synonyms[word]))\n",
    "        else:\n",
    "            synonym_text.append(word)\n",
    "    return ' '.join(synonym_text)\n",
    "\n",
    "def generate_confusing_texts(topics, num_texts=2):\n",
    "    confusing_texts = []\n",
    "    for topic in topics:\n",
    "        for _ in range(num_texts):\n",
    "            text = generator(topic, max_length=300, num_return_sequences=1)[0]['generated_text']\n",
    "            text = introduce_noise(text)\n",
    "            text = replace_with_synonyms(text)\n",
    "            confusing_texts.append(text)\n",
    "    return confusing_texts\n",
    "\n",
    "# Generate confusing texts\n",
    "confusing_texts = generate_confusing_texts(topics)\n",
    "\n",
    "# Create a DataFrame\n",
    "confusing_df = pd.DataFrame({'text': confusing_texts, 'generated': [1] * len(confusing_texts)})\n",
    "\n",
    "# Save to CSV\n",
    "confusing_df.to_csv('confusingGenerated.csv', index=False)\n",
    "\n",
    "# Load the confusingGenerated.csv data\n",
    "confusing_data = pd.read_csv('confusingGenerated.csv')\n",
    "\n",
    "# Preprocess the text data using the same TfidfVectorizer\n",
    "X_confusing = vectorizer.transform(confusing_data['text'])\n",
    "y_confusing = confusing_data['generated']\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "X_confusing_tensor = torch.tensor(X_confusing.toarray(), dtype=torch.float32)\n",
    "y_confusing_tensor = torch.tensor(y_confusing.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Use the trained model to make predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_confusing_pred = model(X_confusing_tensor).numpy()\n",
    "\n",
    "# Calculate MSE and accuracy\n",
    "confusing_mse = mean_squared_error(y_confusing, y_confusing_pred)\n",
    "confusing_accuracy = 1 - confusing_mse\n",
    "\n",
    "print(f\"Confusing Generated Data MSE: {confusing_mse}\")\n",
    "print(f\"Confusing Generated Data Accuracy: {confusing_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
